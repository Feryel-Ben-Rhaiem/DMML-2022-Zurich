# -*- coding: utf-8 -*-
"""BERTModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YpxHJHYLn5ovfxxenVG3CrLdrtzRpllz
"""

# This class is to build and evaluate an NLP model as it should give a better accuracy.
# The model implemented is the Multilingual-base model which extends the BERT transformer architecture.

# Import required packages
import numpy as np
import tensorflow_hub as hub
!pip install tensorflow-text==2.11.0
import tensorflow_text as text
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style("whitegrid") 
import bertfunctions
import datapreparation
from keras import backend as K
from bertfunctions import *
from datapreparation import *

# Load BERT with TensorFlow Hub
preprocessor = hub.KerasLayer("https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2")
encoder = hub.KerasLayer("https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-base/1")

# Create the structure of the classification model
num_classes = len(df["difficulty_num"].value_counts())
i = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
x = preprocessor(i)
x = encoder(x)
x = tf.keras.layers.Dropout(0.2, name="dropout")(x['pooled_output'])
x = tf.keras.layers.Dense(num_classes+1, activation='softmax', name="output")(x)

model = tf.keras.Model(i, x)

# Train the model for 30 epochs
n_epochs = 30

# Define metrics
METRICS = [
      tf.keras.metrics.CategoricalAccuracy(name="accuracy"),
      balanced_recall,
      balanced_precision,
      balanced_f1_score
]

# Use the EarlyStopping callback in order to monitor the validation loss during training
earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor = "val_loss", 
                                                      patience = 3,
                                                      restore_best_weights = True)

# Compile the model 
model.compile(optimizer = "adam",
              loss = "categorical_crossentropy",
              #loss="sparse_categorical_crossentropy",
              metrics = METRICS)

# Fit the model. Nota Bene: we're fitting the model on the whole training set to leverage our data. 
# However, while testing the model, we trained it on 0.8 of the training data and tested it on the other 0.2. 
model_fit = model.fit(df['sentence'], 
                      y_, 
                      epochs = n_epochs,
                      callbacks = [earlystop_callback])

# This is a function wrapping model.predict and returning a numpy.ndarray of size 6 (as we have 6 classes) 
def predict_class(sentences):
  '''predict class of input text
  Args:
    - Sentences (list of strings)
  Output:
    - class (list of int)
  '''
  return [np.argmax(pred) for pred in model.predict(sentences)]

# Predict on the unlabeled data
df_pred['difficulty'] = predict_class(df_pred['sentence']) 

# Show Predictions
predictions = map_difficulty(df_pred, 'difficulty')
print(predictions)
